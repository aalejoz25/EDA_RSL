import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import spacy
from collections import Counter
from itertools import combinations
import itertools
from datetime import datetime
import networkx as nx
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis
import pyLDAvis.lda_model
import re





df = pd.read_csv("referencias_unificado.csv",delimiter='|')


print(df.shape)



df.head(3)



print(df.info())


# Revisión de valores nulos
print(df.isnull().sum())


# normalizar texto 
#Unificar formato para títulos, resúmenes y keywords:
#Pasar a minúsculas para análisis
#Eliminar espacios extra
#Quitar caracteres especiales innecesarios
cols_texto = ["title", "abstract", "author_Keywords", "index_keywords"]

for col in cols_texto:
    df[col] = df[col].astype(str).str.lower().str.strip()
    df[col] = df[col].str.replace(r"\s+", " ", regex=True)


#Convertir year a entero

#Asegurar que cited_by sea numérico

#Rellenar valores nulos con 0 si aplica
df["year"] = pd.to_numeric(df["year"], errors="coerce").fillna(0).astype(int)
df["cited_by"] = pd.to_numeric(df["cited_by"], errors="coerce").fillna(0).astype(int)


#Unificar keywords
#Separar por ; o ,

#Quitar duplicados internos

#Esto facilitará análisis de frecuencia de términos después
def limpiar_keywords(kw):
    if pd.isna(kw):
        return []
    partes = [k.strip() for k in kw.replace(";", ",").split(",")]
    return sorted(set(partes))

df["keywords_list"] = df["author_Keywords"].apply(limpiar_keywords)


# 3. Rellenar valores nulos en columnas de texto clave para evitar errores
df['abstract'] = df['abstract'].fillna('')
df['authors'] = df['authors'].fillna('')
df['DOI'] = df['DOI'].fillna('')
df['author_Keywords'] = df['author_Keywords'].fillna('')
df['index_keywords'] = df['index_keywords'].fillna('')





# validaciones 
sns.set(style="whitegrid", rc={"figure.dpi":150})

# Asegurar columnas mínimas (si faltan, crearlas vacías para evitar errores)
for c in ["fuente","authors","author_full_names","title","year","source_title","cited_by",
          "DOI","abstract","author_Keywords","index_keywords","document_type","open_access"]:
    if c not in df.columns:
        df[c] = np.nan

# info rápida
print("Registros:", df.shape[0])


df[["title","year","source_title"]].head(5)


print(df.isnull().sum().sort_values(ascending=False).head(20))


# calidad de metadatos
# Un resumen rápido
print(df.describe(include='all').T[['count','unique']].head(30))


# % de registros con DOI/abstract/keywords
for col in ["DOI","abstract","author_Keywords","author_full_names"]:
    pct = df[col].notna().sum()/len(df)*100
    print(f"{col}: {df[col].notna().sum()} ({pct:.1f}%)")


#publicaciones por año 
# Asegurar year numérico
df["year"] = pd.to_numeric(df["year"], errors="coerce").astype(pd.Int64Dtype())
pub_by_year = df.dropna(subset=["year"]).groupby("year").size().sort_index()

# Gráfica
plt.figure(figsize=(10,4))
pub_by_year.plot(kind="bar")
plt.title("Publicaciones por año")
plt.xlabel("Año"); plt.ylabel("Número de publicaciones")
plt.tight_layout()
plt.show()
#$plt.savefig("pub_by_year.png")
#$plt.close()

# crecimiento porcentual año a año
pct_change = pub_by_year.pct_change().fillna(0) * 100
pct_change.to_csv("pub_by_year_pct_change.csv")


#fuente
# Preferir 'source_title' si está, sino 'fuente'
src_col = "source_title" if "source_title" in df.columns else "fuente"
top_sources = df[src_col].fillna("Unknown").value_counts().head(30)
top_sources.to_csv("top_sources.csv")

# Índice de concentración (HHI: suma de cuadrados de shares, entre 0 y 1)
shares = df[src_col].value_counts(normalize=True)
hhi = (shares**2).sum()
print("HHI (0-1):", hhi)


# Autores
# Elegir columna con nombres completos si existe
auth_col = "author_full_names" if "author_full_names" in df.columns else "authors"

def split_authors(s):
    if pd.isna(s) or s=="":
        return []
    # Ajusta separadores según tu dataset; cuidado con "Last, First"
    parts = re.split(r";|\||, and | and |\,\s(?=[A-Z])", s)
    return [p.strip() for p in parts if p.strip()]

auth_lists = df[auth_col].fillna("").apply(split_authors)
all_auth = auth_lists.explode().value_counts().head(50)
all_auth.to_csv("top_authors.csv")

# Construir red de coautoría (en nodes se usan solo autores con >=1)
G = nx.Graph()
for authors in auth_lists:
    for i in range(len(authors)):
        for j in range(i+1, len(authors)):
            a,b = authors[i], authors[j]
            if G.has_edge(a,b):
                G[a][b]['weight'] += 1
            else:
                G.add_edge(a,b, weight=1)

# Métricas simples
deg = sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:30]
pd.DataFrame(deg, columns=["author","degree"]).to_csv("coauthor_degree_top30.csv")
print("Nodos coautoría:", G.number_of_nodes(), "Aristas:", G.number_of_edges())


#Keywords — frecuencia y co-ocurrencia
# Unir keywords de ambas columnas si existen
keywords_series = (
    df['author_Keywords'].fillna('') + ';' + df['index_keywords'].fillna('')
)

# Dividir por separador y limpiar
keywords = []
for row in keywords_series:
    for kw in str(row).split(';'):
        kw_clean = kw.strip().lower()
        if kw_clean:
            keywords.append(kw_clean)

# Contar frecuencia
counter = Counter(keywords)
top_keywords = counter.most_common(20)

# DataFrame para gráfico
kw_df = pd.DataFrame(top_keywords, columns=['keyword', 'freq'])

# Visualizar
plt.figure(figsize=(10,6))
sns.barplot(y='keyword', x='freq', data=kw_df, palette='viridis')
plt.title("Top 20 Keywords más frecuentes", fontsize=14)
plt.xlabel("Frecuencia")
plt.ylabel("Keyword")
plt.show()


#Co-ocurrencia de Keywords
#Objetivo: detectar qué términos aparecen juntos con más frecuencia.
# Crear pares de co-ocurrencia
cooc_pairs = Counter()
for row in keywords_series:
    kws = list(set([kw.strip().lower() for kw in str(row).split(';') if kw.strip()]))
    for combo in combinations(kws, 2):
        cooc_pairs[tuple(sorted(combo))] += 1

# Grafo de co-ocurrencias
G = nx.Graph()
for (kw1, kw2), freq in cooc_pairs.items():
    if freq >= 3:  # filtro para no saturar
        G.add_edge(kw1, kw2, weight=freq)

# Dibujar grafo
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G, k=0.5)
nx.draw_networkx_nodes(G, pos, node_size=300, node_color='skyblue')
nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']*0.2 for u,v in G.edges()])
nx.draw_networkx_labels(G, pos, font_size=8)
plt.title("Co-ocurrencia de Keywords (≥3 veces)")
plt.axis('off')
plt.show()


#N-gramas y términos en Abstracts
#Objetivo: encontrar frases comunes y términos recurrentes en los resúmenes.
# Unir todos los abstracts
abstracts = df['abstract'].fillna('').str.lower()

# N-gramas (2 y 3 palabras)
vectorizer = CountVectorizer(ngram_range=(2,3), stop_words='english').fit(abstracts)
X = vectorizer.transform(abstracts)
sum_words = X.sum(axis=0)

ngrams_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
ngrams_freq = sorted(ngrams_freq, key=lambda x: x[1], reverse=True)[:20]

# DataFrame
ngram_df = pd.DataFrame(ngrams_freq, columns=['ngram', 'freq'])

# Visualizar
plt.figure(figsize=(10,6))
sns.barplot(y='ngram', x='freq', data=ngram_df, palette='mako')
plt.title("Top 20 N-gramas más frecuentes en Abstracts", fontsize=14)
plt.xlabel("Frecuencia")
plt.ylabel("N-grama")
plt.show()


# Normalizamos año
df['year'] = pd.to_numeric(df['year'], errors='coerce')
df = df.dropna(subset=['year'])

# Expandir keywords con año
rows = []
for _, row in df.iterrows():
    kws = (str(row['author_Keywords']) + ';' + str(row['index_keywords'])).split(';')
    for kw in kws:
        kw_clean = kw.strip().lower()
        if kw_clean:
            rows.append({'year': int(row['year']), 'keyword': kw_clean})

kw_year_df = pd.DataFrame(rows)

# Contar frecuencia por año
trend = kw_year_df.groupby(['keyword', 'year']).size().reset_index(name='count')

# Calcular crecimiento relativo (últimos 3 años vs resto)
latest_years = trend['year'].max() - 2
growth = (
    trend[trend['year'] >= latest_years]
    .groupby('keyword')['count'].sum()
    / trend[trend['year'] < latest_years]
    .groupby('keyword')['count'].sum()
)

growth = growth.sort_values(ascending=False).dropna().head(10)

# Visualizar crecimiento
growth.plot(kind='bar', figsize=(10,5), color='orange')
plt.title("Top 10 Keywords con mayor crecimiento reciente")
plt.ylabel("Ratio de crecimiento")
plt.show()


# ========================
# 1. Producción por año
# ========================
pub_per_year = df.groupby('year').size()

plt.figure(figsize=(10, 5))
sns.lineplot(x=pub_per_year.index, y=pub_per_year.values, marker='o')
plt.title("Producción de publicaciones por año")
plt.xlabel("Año")
plt.ylabel("Número de publicaciones")
plt.grid(True)
plt.show()


# ========================
# 2. Tendencias de palabras clave más frecuentes
# ========================

# Función para limpiar y separar palabras clave
def clean_keywords(text):
    if pd.isna(text):
        return []
    # Separar por ; o , y limpiar espacios
    words = re.split(r';|,', text)
    return [w.strip().lower() for w in words if w.strip()]

# Expandir keywords en filas
df_keywords = df[['year', 'author_Keywords']].dropna()
df_keywords['keywords_list'] = df_keywords['author_Keywords'].apply(clean_keywords)

all_keywords = [kw for kws in df_keywords['keywords_list'] for kw in kws]
top_keywords = [kw for kw, _ in Counter(all_keywords).most_common(5)]

# Crear DataFrame de tendencias
trend_data = []
for year, group in df_keywords.groupby('year'):
    kws_year = [kw for kws in group['keywords_list'] for kw in kws]
    counts = Counter(kws_year)
    for kw in top_keywords:
        trend_data.append({'year': year, 'keyword': kw, 'count': counts.get(kw, 0)})

trend_df = pd.DataFrame(trend_data)

plt.figure(figsize=(12, 6))
sns.lineplot(data=trend_df, x='year', y='count', hue='keyword', marker='o')
plt.title("Tendencia de las palabras clave más frecuentes")
plt.xlabel("Año")
plt.ylabel("Frecuencia")
plt.grid(True)
plt.show()


# Agrupar por año y contar
pubs_por_anio = df.groupby("year")["title"].count()

# Graficar
plt.figure(figsize=(10,5))
pubs_por_anio.plot(kind='bar', color='skyblue')
plt.title("Tendencia de publicaciones por año", fontsize=14)
plt.xlabel("Año")
plt.ylabel("Cantidad de publicaciones")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Crear grafo
G = nx.Graph()

for autores in df['authors'].dropna():
    lista_autores = [a.strip() for a in autores.split(";")]
    for i in range(len(lista_autores)):
        for j in range(i+1, len(lista_autores)):
            G.add_edge(lista_autores[i], lista_autores[j])

# Dibujar red (simplificada para autores con más de 3 artículos)
autores_frecuentes = [a for a, count in top_autores.items()]
H = G.subgraph(autores_frecuentes)

plt.figure(figsize=(8,8))
nx.draw(H, with_labels=True, node_size=500, node_color="lightblue", font_size=8)
plt.title("Red de coautoría (autores más prolíficos)", fontsize=14)
plt.show()


G = nx.Graph()

for autores in df['authors'].dropna():
    lista_autores = [a.strip() for a in autores.split(";")]
    for i in range(len(lista_autores)):
        for j in range(i+1, len(lista_autores)):
            if G.has_edge(lista_autores[i], lista_autores[j]):
                G[lista_autores[i]][lista_autores[j]]['weight'] += 1
            else:
                G.add_edge(lista_autores[i], lista_autores[j], weight=1)


degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G)

# Top 5 autores por cada métrica
print("Top 5 por grado:", sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5])
print("Top 5 por intermediación:", sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5])
print("Top 5 por eigenvector:", sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5])


# Nos aseguramos de que las columnas necesarias existan
for col in ["title", "abstract", "author_Keywords", "year"]:
    if col not in df.columns:
        raise ValueError(f"Falta la columna '{col}' en el CSV.")


# ====== 2. Unir texto ======
df["all_text"] = (
    df["title"].fillna("") + " " +
    df["abstract"].fillna("") + " " +
    df["author_Keywords"].fillna("")
)


nltk.download('stopwords')
stop_words = set(stopwords.words('english') + stopwords.words('spanish'))

# ====== 3. Función de limpieza ======
def limpiar_texto(text):
    text = text.lower()
    text = re.sub(r"[^a-záéíóúüñ\s]", "", text)  # letras y espacios
    tokens = [word for word in text.split() if word not in stop_words and len(word) > 2]
    return " ".join(tokens)

df["clean_text"] = df["all_text"].apply(limpiar_texto)


# ====== 4. Palabras más frecuentes ======
all_words = " ".join(df["clean_text"]).split()
word_freq = Counter(all_words)
common_words = word_freq.most_common(20)


# ====== 5. Nube de palabras ======
wc = WordCloud(width=1000, height=600, background_color="white").generate(" ".join(all_words))
plt.figure(figsize=(10,6))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Nube de palabras - Temas principales", fontsize=16)
plt.show()


# ====== 6. Barra de términos más frecuentes ======
words, freqs = zip(*common_words)
plt.figure(figsize=(10,6))
plt.barh(words[::-1], freqs[::-1], color="skyblue")
plt.title("Términos más frecuentes")
plt.xlabel("Frecuencia")
plt.show()


# ====== 8. Evolución anual de palabras clave ======
df["kw_list"] = df["author_Keywords"].fillna("").apply(lambda x: [k.strip().lower() for k in x.split(";") if k.strip()])
df["year"] = pd.to_numeric(df["year"], errors="coerce")
year_kw = {}
for _, row in df.iterrows():
    year = row["year"]
    for kw in row["kw_list"]:
        year_kw.setdefault(kw, {}).setdefault(year, 0)
        year_kw[kw][year] += 1

# Seleccionamos top 5 keywords por frecuencia total
total_kw = Counter({kw: sum(freqs.values()) for kw, freqs in year_kw.items()})
top_keywords = [kw for kw, _ in total_kw.most_common(5)]

plt.figure(figsize=(10,6))
for kw in top_keywords:
    years = sorted(year_kw[kw].keys())
    counts = [year_kw[kw][y] for y in years]
    plt.plot(years, counts, marker="o", label=kw)

plt.title("Evolución anual de las palabras clave más usadas")
plt.xlabel("Año")
plt.ylabel("Frecuencia")
plt.legend()
plt.show()


# Combinar columnas relevantes (evitar NaN)
df["text"] = df[["title", "abstract", "author_Keywords"]].fillna("").agg(" ".join, axis=1)

# --------------------
# 2. Limpieza y preprocesamiento
# --------------------
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])  # Usa modelo inglés
stopwords_extra = {"study", "results", "paper", "using", "based"}  # Palabras poco informativas

def limpiar_texto(text):
    text = text.lower()
    text = re.sub(r'[^a-záéíóúüñ ]', ' ', text)  # Solo letras y espacios
    doc = nlp(text)
    tokens = [tok.lemma_ for tok in doc 
              if tok.is_alpha and tok.lemma_ not in stopwords_extra and not tok.is_stop]
    return " ".join(tokens)

df["clean_text"] = df["text"].apply(limpiar_texto)

# --------------------
# 3. Vectorización
# --------------------
vectorizer = CountVectorizer(max_df=0.8, min_df=3)  
X = vectorizer.fit_transform(df["clean_text"])

# --------------------
# 4. Entrenar LDA
# --------------------
n_topics = 8  # Ajusta el número de temas
lda = LatentDirichletAllocation(n_components=n_topics, 
                                random_state=42,
                                learning_method="batch")
lda.fit(X)

# --------------------
# 5. Mostrar palabras por tema
# --------------------
def mostrar_temas(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        print(f"Tema {topic_idx+1}: {' | '.join(top_words)}")

mostrar_temas(lda, vectorizer.get_feature_names_out(), 10)

# --------------------
# 6. Visualización pyLDAvis
# --------------------
pyLDAvis.enable_notebook()
panel = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne')
pyLDAvis.save_html(panel, "LDA_resultados.html")

print("\n✅ Resultados guardados en 'LDA_resultados.html' para exploración interactiva.")




















# --- Preparación para Análisis de Texto ---
# Descargar la lista de stopwords (palabras comunes como 'the', 'is', 'in')
# Esto solo se necesita ejecutar una vez
nltk.download('stopwords')
stop_words = set(stopwords.words('english')) # Puedes añadir 'spanish' si aplica


print(f"Dataset cargado con {df.shape[0]} artículos únicos.")


# Contar publicaciones por año
publications_per_year = df['year'].value_counts().sort_index()

# Visualizar
plt.figure(figsize=(12, 6))
sns.barplot(x=publications_per_year.index, y=publications_per_year.values, color='skyblue')
plt.title('Número de Publicaciones por Año')
plt.xlabel('Año')
plt.ylabel('Cantidad de Artículos')
plt.xticks(rotation=45)
plt.show()


# Contar las 15 fuentes más comunes
top_sources = df['source_title'].value_counts().nlargest(15)

# Visualizar
plt.figure(figsize=(10, 8))
sns.barplot(x=top_sources.values, y=top_sources.index, orient='h')
plt.title('Top 15 Revistas y Congresos')
plt.xlabel('Cantidad de Artículos')
plt.show()


# La columna 'authors' suele ser un string separado por punto y coma o comas
# Creamos una lista de todos los autores
all_authors = df['authors'].str.split(';').explode()
all_authors = all_authors.str.strip() # Limpiar espacios en blanco

# Contar los 15 autores más frecuentes
top_authors = all_authors.value_counts().nlargest(15)
print("Top 15 Autores Más Prolíficos:")
print(top_authors)


# Procesar la columna de palabras clave
all_keywords = df['author_Keywords'].str.split(';').explode()
all_keywords = all_keywords.str.strip().str.lower() # Normalizar

# Generar nube de palabras
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_keywords.dropna()))

plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de Palabras Clave de Autores')
plt.show()


# Procesar la columna de palabras clave
all_keywords = df['index_keywords'].str.split(';').explode()
all_keywords = all_keywords.str.strip().str.lower() # Normalizar

# Generar nube de palabras
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_keywords.dropna()))

plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de Palabras Clave de index')
plt.show()


# Combinar títulos y resúmenes en un solo texto por documento
df['text_for_analysis'] = df['title'] + ' ' + df['abstract']
# Limpiar el texto: minúsculas y quitar no alfanuméricos
df['text_for_analysis'] = df['text_for_analysis'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', x))


# --- Encontrar Bigramas (pares de palabras) más comunes ---
vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), stop_words=list(stop_words))
X_bigram = vectorizer_bigram.fit_transform(df['text_for_analysis'])
bigram_counts = pd.DataFrame({
    'bigram': vectorizer_bigram.get_feature_names_out(),
    'count': X_bigram.toarray().sum(axis=0)
}).sort_values('count', ascending=False).head(20)


print("Top 20 Bigramas Más Comunes:")
print(bigram_counts)


# Usaremos la columna 'text_for_analysis' que creamos antes
# (título + resumen, en minúsculas y sin stopwords)

# Vectorizador: Ignora palabras que aparecen en más del 95% de los docs
# o en menos de 2 documentos. Esto elimina ruido.
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=list(stop_words))

# Aplica el vectorizador para crear la matriz de frecuencias de palabras
X = vectorizer.fit_transform(df['text_for_analysis'])


# Define el número de tópicos que quieres encontrar (esto es un hiperparámetro clave)
num_topics = 10 # Empezamos con 10 como una suposición educada

# Crea y entrena el modelo LDA
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(X)


# Función para mostrar los tópicos
def display_topics(model, feature_names, num_top_words):
    for topic_idx, topic in enumerate(model.components_):
        # Imprime el número del tópico y las palabras más representativas
        print(f"Tópico #{topic_idx+1}:")
        message = " ".join([feature_names[i]
                            for i in topic.argsort()[:-num_top_words - 1:-1]])
        print(message)
        print("-" * 50)

# Obtiene los nombres de las palabras (las columnas de nuestra matriz)
feature_names = vectorizer.get_feature_names_out()
num_top_words = 10

# Muestra los tópicos
display_topics(lda, feature_names, num_top_words)


# Generar la visualización
# Nota: es mejor hacerlo en un entorno como Jupyter Notebook para verla
pyLDAvis.enable_notebook()
panel = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne')

# Mostrar el panel interactivo
panel






