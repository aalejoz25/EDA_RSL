fuente,authors,author_full_names,Author_ID,title,year,source_title,volume,issue,art,page_start,page_end,page_count,cited_by,DOI,link,abstract,author_Keywords,index_keywords,document_type,publication_stage,open_access,source,EID,text_for_analysis,keywords_list,age,cit_per_year
scopus,Wang J.; Huang Y.; Chen C.; Liu Z.; Wang S.; Wang Q.,"Wang, Junjie (55976866600); Huang, Yuchao (57225204798); Chen, Chunyang (57191225906); Liu, Zhe (57221434139); Wang, Song (56995463200); Wang, Qing (55698296000)",55976866600; 57225204798; 57191225906; 57221434139; 56995463200; 55698296000,"software testing with large language models: survey, landscape, and vision",2024,IEEE Transactions on Software Engineering,50,4,,911,936,25,161,10.1109/TSE.2024.3368208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187981851&doi=10.1109%2fTSE.2024.3368208&partnerID=40&md5=b71952c4e45ad7ff9a41d511f92a830e,"pre-trained large language models (llms) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. as the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of llms. this paper provides a comprehensive review of the utilization of llms in software testing. it analyzes 102 relevant studies that have used llms for software testing, from both the software testing and llms perspectives. the paper presents a detailed discussion of the software testing tasks for which llms are commonly used, among which test case preparation and program repair are the most representative. it also analyzes the commonly used llms, the types of prompt engineering that are employed, as well as the accompanied techniques with these llms. it also summarizes the key challenges and potential opportunities in this direction. this work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of llms in software testing. © 1976-2012 ieee.",gpt; llm; pre-trained large language model; software testing,computational linguistics; engineering research; large datasets; natural language processing systems; software reliability; breakthrough technology; gpt; language model; language processing; large language model; large-scale datasets; natural languages; pre-trained large language model; software testings; software testing,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85187981851,software testing with large language models survey landscape and vision pretrained large language models llms have recently emerged as a breakthrough technology in natural language processing and artificial intelligence with the ability to handle largescale datasets and exhibit remarkable performance across a wide range of tasks meanwhile software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products as the scope and complexity of software systems continue to grow the need for more effective software testing techniques becomes increasingly urgent making it an area ripe for innovative approaches such as the use of llms this paper provides a comprehensive review of the utilization of llms in software testing it analyzes  relevant studies that have used llms for software testing from both the software testing and llms perspectives the paper presents a detailed discussion of the software testing tasks for which llms are commonly used among which test case preparation and program repair are the most representative it also analyzes the commonly used llms the types of prompt engineering that are employed as well as the accompanied techniques with these llms it also summarizes the key challenges and potential opportunities in this direction this work can serve as a roadmap for future research in this area highlighting potential avenues for exploration and identifying gaps in our current understanding of the use of llms in software testing   ieee,"['gpt', 'llm', 'pre-trained large language model', 'software testing']",2,80.5
scopus,Jalil S.; Rafi S.; Latoza T.D.; Moran K.; Lam W.,"Jalil, Sajed (58109615000); Rafi, Suzzana (57215138802); Latoza, Thomas D. (16230457100); Moran, Kevin (57095532500); Lam, Wing (56879749600)",58109615000; 57215138802; 16230457100; 57095532500; 56879749600,chatgpt and software testing education: promises & perils,2023,"Proceedings - 2023 IEEE 16th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2023",,,,430,437,7,159,10.1109/ICSTW58534.2023.00078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163136873&doi=10.1109%2fICSTW58534.2023.00078&partnerID=40&md5=a2834eaacaaf7ddfd6590f9b3d9c759e,"over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. more recently, we have seen the ad-vent of general purpose ""large language models"", based on neural transformer architectures, that have been trained on massive datasets of human written text, which includes code and natural language. however, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. many of these limitations were recently overcome with the introduction of chatgpt, a language model created by openai and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users.the introduction of models, such as chatgpt, has already spurred fervent discussion from educators, ranging from fear that students could use these ai tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. however, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. as such, in this paper, we examine how well chatgpt performs when tasked with answering common questions in a popular software testing curriculum. we found that given its current capabilities, chatgpt is able to respond to 77.5% of the questions we examined and that, of these questions, it is able to provide correct or partially correct answers in 55.6% of cases, provide correct or partially correct explanations of answers in 53.0% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers and explanations. based on these findings, we discuss the potential promises and perils related to the use of chatgpt by students and instructors. © 2023 ieee.",case study; chatgpt; education; testing,codes (symbols); computational linguistics; curricula; education computing; large dataset; modeling languages; natural language processing systems; statistical tests; students; well testing; case-studies; chatgpt; code languages; language model; massive data sets; model-based opc; natural languages; new forms; software testings; written texts; software testing,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85163136873,chatgpt and software testing education promises  perils over the past decade predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers more recently we have seen the advent of general purpose large language models based on neural transformer architectures that have been trained on massive datasets of human written text which includes code and natural language however despite the demonstrated representational power of such models interacting with them has historically been constrained to specific task settings limiting their general applicability many of these limitations were recently overcome with the introduction of chatgpt a language model created by openai and trained to operate as a conversational agent enabling it to answer questions and respond to a wide variety of commands from end usersthe introduction of models such as chatgpt has already spurred fervent discussion from educators ranging from fear that students could use these ai tools to circumvent learning to excitement about the new types of learning opportunities that they might unlock however given the nascent nature of these tools we currently lack fundamental knowledge related to how well they perform in different educational settings and the potential promise or danger that they might pose to traditional forms of instruction as such in this paper we examine how well chatgpt performs when tasked with answering common questions in a popular software testing curriculum we found that given its current capabilities chatgpt is able to respond to  of the questions we examined and that of these questions it is able to provide correct or partially correct answers in  of cases provide correct or partially correct explanations of answers in  of cases and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers and explanations based on these findings we discuss the potential promises and perils related to the use of chatgpt by students and instructors   ieee,"['case study', 'chatgpt', 'education', 'testing']",3,53.0
scopus,Thota M.K.; Shajin F.H.; Rajesh P.,"Thota, Mahesh Kumar (57219612355); Shajin, Francis H (57218325222); Rajesh, P. (58954359700)",57219612355; 57218325222; 58954359700,survey on software defect prediction techniques,2020,International Journal of Applied Science and Engineering,17,4,,331,344,13,286,10.6703/IJASE.202012_17(4).331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100834217&doi=10.6703%2fIJASE.202012_17%284%29.331&partnerID=40&md5=2a49e223432655d9aebd846927bfab0e,"recent advancements in technology have emerged the requirements of hardware and software applications. along with this technical growth, software industries also have faced drastic growth in the demand of software for several applications. for any software industry, developing good quality software and maintaining its eminence for user end is considered as most important task for software industrial growth. in order to achieve this, software engineering plays an important role for software industries. software applications are developed with the help of computer programming where codes are written for desired task. generally, these codes contain some faulty instances which may lead to the buggy software development cause due to software defects. in the field of software engineering, software defect prediction is considered as most important task which can be used for maintaining the quality of software. defect prediction results provide the list of defect-prone source code artefacts so that quality assurance team scan effectively allocate limited resources for validating software products by putting more effort on the defect-prone source code. as the size of software projects becomes larger, defect prediction techniques will play an important role to support developers as well as to speed up time to market with more reliable software products. one of the most exhaustive and pricey part of embedded software development is consider as the process of finding and fixing the defects. due to complex infrastructure, magnitude, cost and time limitations, monitoring and fulfilling the quality is a big challenge, especially in automotive embedded systems. however, meeting the superior product quality and reliability is mandatory. hence, higher importance is given to v&v (verification & validation). software testing is an integral part of software v&v, which is focused on promising accurate functionality and long-term reliability of software systems. simultaneously, software testing requires much effort, cost, infrastructure and expertise as the development. the costs and efforts elevate in safety critical software systems. therefore, it is essential to have a good testing strategy for any industry with high software development costs. in this work, we are planning to develop an efficient approach for software defect prediction by using soft computing based machine learning techniques which helps to predict optimize the features and efficiently learn the features. © the author(s). this is an open access article distributed under the terms of the creative commons attribution license (cc by 4.0), which permits unrestricted distribution provided the original author and source are cited.",defect prediction; soft computing; validation; verification,,Article,Final,,Scopus,2-s2.0-85100834217,survey on software defect prediction techniques recent advancements in technology have emerged the requirements of hardware and software applications along with this technical growth software industries also have faced drastic growth in the demand of software for several applications for any software industry developing good quality software and maintaining its eminence for user end is considered as most important task for software industrial growth in order to achieve this software engineering plays an important role for software industries software applications are developed with the help of computer programming where codes are written for desired task generally these codes contain some faulty instances which may lead to the buggy software development cause due to software defects in the field of software engineering software defect prediction is considered as most important task which can be used for maintaining the quality of software defect prediction results provide the list of defectprone source code artefacts so that quality assurance team scan effectively allocate limited resources for validating software products by putting more effort on the defectprone source code as the size of software projects becomes larger defect prediction techniques will play an important role to support developers as well as to speed up time to market with more reliable software products one of the most exhaustive and pricey part of embedded software development is consider as the process of finding and fixing the defects due to complex infrastructure magnitude cost and time limitations monitoring and fulfilling the quality is a big challenge especially in automotive embedded systems however meeting the superior product quality and reliability is mandatory hence higher importance is given to vv verification  validation software testing is an integral part of software vv which is focused on promising accurate functionality and longterm reliability of software systems simultaneously software testing requires much effort cost infrastructure and expertise as the development the costs and efforts elevate in safety critical software systems therefore it is essential to have a good testing strategy for any industry with high software development costs in this work we are planning to develop an efficient approach for software defect prediction by using soft computing based machine learning techniques which helps to predict optimize the features and efficiently learn the features  the authors this is an open access article distributed under the terms of the creative commons attribution license cc by  which permits unrestricted distribution provided the original author and source are cited,"['defect prediction', 'soft computing', 'validation', 'verification']",6,47.666666666666664
scopus,Martínez-Fernández S.; Bogner J.; Franch X.; Oriol M.; Siebert J.; Trendowicz A.; Vollmer A.M.; Wagner S.,"Martínez-Fernández, Silverio (55363648100); Bogner, Justus (57189261793); Franch, Xavier (6603081752); Oriol, Marc (53880191200); Siebert, Julien (57219057225); Trendowicz, Adam (12762670400); Vollmer, Anna Maria (57197746321); Wagner, Stefan (55286051900)",55363648100; 57189261793; 6603081752; 53880191200; 57219057225; 12762670400; 57197746321; 55286051900,software engineering for ai-based systems: a survey,2022,ACM Transactions on Software Engineering and Methodology,31,2,37e,,,,171,10.1145/3487043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727000&doi=10.1145%2f3487043&partnerID=40&md5=fbf7ac61842908f81cd945ae6a1009e4,"ai-based systems are software systems with functionalities enabled by at least one ai component (e.g., for image-, speech-recognition, and autonomous driving). ai-based systems are becoming pervasive in society due to advances in ai. however, there is limited synthesized knowledge on software engineering (se) approaches for building, operating, and maintaining ai-based systems. to collect and analyze state-of-the-art knowledge about se for ai-based systems, we conducted a systematic mapping study. we considered 248 studies published between january 2010 and march 2020. se for ai-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. the most studied properties of ai-based systems are dependability and safety. we identified multiple se approaches for ai-based systems, which we classified according to the swebok areas. studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. data-related issues are the most recurrent challenges. our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that se entails for ai-based systems; and, educators, to bridge the gap among se and ai in their curricula. © 2022 copyright held by the owner/author(s).",ai-based systems; artificial intelligence; software engineering; systematic mapping study,application programs; artificial intelligence; computer software selection and evaluation; curricula; software testing; speech recognition; ai-based system; autonomous driving; classifieds; learn+; property; research areas; software-systems; state of the art; synthesised; systematic mapping studies; mapping,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85130727000,software engineering for aibased systems a survey aibased systems are software systems with functionalities enabled by at least one ai component eg for image speechrecognition and autonomous driving aibased systems are becoming pervasive in society due to advances in ai however there is limited synthesized knowledge on software engineering se approaches for building operating and maintaining aibased systems to collect and analyze stateoftheart knowledge about se for aibased systems we conducted a systematic mapping study we considered  studies published between january  and march  se for aibased systems is an emerging research area where more than  of the studies have been published since  the most studied properties of aibased systems are dependability and safety we identified multiple se approaches for aibased systems which we classified according to the swebok areas studies related to software testing and software quality are very prevalent while areas like software maintenance seem neglected datarelated issues are the most recurrent challenges our results are valuable for researchers to quickly understand the stateoftheart and learn which topics need more research practitioners to learn about the approaches and challenges that se entails for aibased systems and educators to bridge the gap among se and ai in their curricula    copyright held by the ownerauthors,"['ai-based systems', 'artificial intelligence', 'software engineering', 'systematic mapping study']",4,42.75
scopus,Alon-Barkat S.; Busuioc M.,"Alon-Barkat, Saar (56922265900); Busuioc, Madalina (33567608200)",56922265900; 33567608200,human-ai interactions in public sector decision making: “automation bias” and “selective adherence” to algorithmic advice,2023,Journal of Public Administration Research and Theory,33,1,,153,169,16,122,10.1093/jopart/muac007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193223068&doi=10.1093%2fjopart%2fmuac007&partnerID=40&md5=22d8f72fd2ebd9285594db057e9bb5aa,"artificial intelligence algorithms are increasingly adopted as decisional aides by public bodies, with the promise of overcoming biases of human decision-makers. at the same time, they may introduce new biases in the human-algorithm interaction. drawing on psychology and public administration literatures, we investigate two key biases: overreliance on algorithmic advice even in the face of “warning signals” from other sources (automation bias), and selective adoption of algorithmic advice when this corresponds to stereotypes (selective adherence). we assess these via three experimental studies conducted in the netherlands: in study 1 (n = 605), we test automation bias by exploring participants' adherence to an algorithmic prediction compared to an equivalent human-expert prediction. we do not find evidence for automation bias. in study 2 (n = 904), we replicate these findings, and also test selective adherence. we find a stronger propensity for adherence when the advice is aligned with group stereotypes, with no significant differences between algorithmic and human-expert advice. in study 3 (n = 1,345), we replicate our design with a sample of civil servants. this study was conducted shortly after a major scandal involving public authorities' reliance on an algorithm with discriminatory outcomes (the “childcare benefits scandal”). the scandal is itself illustrative of our theory and patterns diagnosed empirically in our experiment, yet in our study 3, while supporting our prior findings as to automation bias, we do not find patterns of selective adherence. we suggest this is driven by bureaucrats' enhanced awareness of discrimination and algorithmic biases in the aftermath of the scandal. we discuss the implications of our findings for public sector decision making in the age of automation. overall, our study speaks to potential negative effects of automation of the administrative state for already vulnerable and disadvantaged citizens. © the author(s) 2022.",,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85193223068,humanai interactions in public sector decision making automation bias and selective adherence to algorithmic advice artificial intelligence algorithms are increasingly adopted as decisional aides by public bodies with the promise of overcoming biases of human decisionmakers at the same time they may introduce new biases in the humanalgorithm interaction drawing on psychology and public administration literatures we investigate two key biases overreliance on algorithmic advice even in the face of warning signals from other sources automation bias and selective adoption of algorithmic advice when this corresponds to stereotypes selective adherence we assess these via three experimental studies conducted in the netherlands in study  n   we test automation bias by exploring participants adherence to an algorithmic prediction compared to an equivalent humanexpert prediction we do not find evidence for automation bias in study  n   we replicate these findings and also test selective adherence we find a stronger propensity for adherence when the advice is aligned with group stereotypes with no significant differences between algorithmic and humanexpert advice in study  n   we replicate our design with a sample of civil servants this study was conducted shortly after a major scandal involving public authorities reliance on an algorithm with discriminatory outcomes the childcare benefits scandal the scandal is itself illustrative of our theory and patterns diagnosed empirically in our experiment yet in our study  while supporting our prior findings as to automation bias we do not find patterns of selective adherence we suggest this is driven by bureaucrats enhanced awareness of discrimination and algorithmic biases in the aftermath of the scandal we discuss the implications of our findings for public sector decision making in the age of automation overall our study speaks to potential negative effects of automation of the administrative state for already vulnerable and disadvantaged citizens  the authors ,[''],3,40.666666666666664
scopus,Riccio V.; Jahangirova G.; Stocco A.; Humbatova N.; Weiss M.; Tonella P.,"Riccio, Vincenzo (57214054052); Jahangirova, Gunel (57190973501); Stocco, Andrea (36882807000); Humbatova, Nargiz (57219011768); Weiss, Michael (57198623419); Tonella, Paolo (7003489194)",57214054052; 57190973501; 36882807000; 57219011768; 57198623419; 7003489194,testing machine learning based systems: a systematic mapping,2020,Empirical Software Engineering,25,6,,5193,5254,61,182,10.1007/s10664-020-09881-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091048368&doi=10.1007%2fs10664-020-09881-0&partnerID=40&md5=834dbb1627e3ff6b79e724005b1aba95,"context:: a machine learning based system (mls) is a software system including one or more components that learn how to perform a task from a given data set. the increasing adoption of mlss in safety critical domains such as autonomous driving, healthcare, and finance has fostered much attention towards the quality assurance of such systems. despite the advances in software testing, mlss bring novel and unprecedented challenges, since their behaviour is defined jointly by the code that implements them and the data used for training them. objective:: to identify the existing solutions for functional testing of mlss, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. to report demographic information about the ongoing research. to identify open challenges for future research. method:: we conducted a systematic mapping study about testing techniques for mlss driven by 33 research questions. we followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results. results:: we identified 70 relevant primary studies, mostly published in the last years. we identified 11 problems addressed in the literature. we investigated multiple aspects of the testing approaches, such as the used/proposed adequacy criteria, the algorithms for test input generation, and the test oracles. conclusions:: the most active research areas in mls testing address automated scenario/input generation and test oracle creation. mls testing is a rapidly growing and developing research area, with many open challenges, such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks. © 2020, the author(s).",machine learning; software testing; systematic mapping; systematic review,machine learning; mapping; quality assurance; safety engineering; testing; turing machines; autonomous driving; demographic information; empirical evaluations; functional testing; research questions; safety-critical domain; systematic mapping; systematic mapping studies; software testing,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85091048368,testing machine learning based systems a systematic mapping context a machine learning based system mls is a software system including one or more components that learn how to perform a task from a given data set the increasing adoption of mlss in safety critical domains such as autonomous driving healthcare and finance has fostered much attention towards the quality assurance of such systems despite the advances in software testing mlss bring novel and unprecedented challenges since their behaviour is defined jointly by the code that implements them and the data used for training them objective to identify the existing solutions for functional testing of mlss and classify them from three different perspectives  the context of the problem they address  their features and  their empirical evaluation to report demographic information about the ongoing research to identify open challenges for future research method we conducted a systematic mapping study about testing techniques for mlss driven by  research questions we followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results results we identified  relevant primary studies mostly published in the last years we identified  problems addressed in the literature we investigated multiple aspects of the testing approaches such as the usedproposed adequacy criteria the algorithms for test input generation and the test oracles conclusions the most active research areas in mls testing address automated scenarioinput generation and test oracle creation mls testing is a rapidly growing and developing research area with many open challenges such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks   the authors,"['machine learning', 'software testing', 'systematic mapping', 'systematic review']",6,30.333333333333332
scopus,Braiek H.B.; Khomh F.,"Braiek, Houssem Ben (57203412343); Khomh, Foutse (24724747600)",57203412343; 24724747600,on testing machine learning programs,2020,Journal of Systems and Software,164,,110542,,,,134,10.1016/j.jss.2020.110542,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081030171&doi=10.1016%2fj.jss.2020.110542&partnerID=40&md5=0019441a753439c6d2a0513d3a102029,"nowadays, we are witnessing a wide adoption of machine learning (ml) models in many software systems. they are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. many people are now interacting with systems based on ml every day, e.g., voice recognition systems used by virtual personal assistants like amazon alexa or google home. as the field of ml continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. given this growing importance of ml-based systems in our daily life, it is becoming utterly important to ensure their reliability. recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ml engineers detect and correct faults in ml programs. this paper reviews current existing testing practices for ml programs. first, we identify and explain challenges that should be addressed when testing ml programs. next, we report existing solutions found in the literature for testing ml programs. finally, we identify gaps in the literature related to the testing of ml programs and make recommendations of future research directions for the scientific community. we hope that this comprehensive review of software testing practices will help ml engineers identify the right approach to improve the reliability of their ml-based systems. we also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ml programs. © 2020",data cleaning; feature engineering testing; implementation testing; machine learning; model testing,deep learning; learning systems; reinforcement learning; safety engineering; software reliability; data cleaning; feature engineerings; future research directions; implementation testing; model testing; property based testing; safety critical systems; voice-recognition systems; software testing,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85081030171,on testing machine learning programs nowadays we are witnessing a wide adoption of machine learning ml models in many software systems they are even being tested in safetycritical systems thanks to recent breakthroughs in deep learning and reinforcement learning many people are now interacting with systems based on ml every day eg voice recognition systems used by virtual personal assistants like amazon alexa or google home as the field of ml continues to grow we are likely to witness transformative advances in a wide range of areas from finance energy to health and transportation given this growing importance of mlbased systems in our daily life it is becoming utterly important to ensure their reliability recently software researchers have started adapting concepts from the software testing domain eg code coverage mutation testing or propertybased testing to help ml engineers detect and correct faults in ml programs this paper reviews current existing testing practices for ml programs first we identify and explain challenges that should be addressed when testing ml programs next we report existing solutions found in the literature for testing ml programs finally we identify gaps in the literature related to the testing of ml programs and make recommendations of future research directions for the scientific community we hope that this comprehensive review of software testing practices will help ml engineers identify the right approach to improve the reliability of their mlbased systems we also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ml programs  ,"['data cleaning', 'feature engineering testing', 'implementation testing', 'machine learning', 'model testing']",6,22.333333333333332
scopus,Zong P.; Lv T.; Wang D.; Deng Z.; Liang R.; Chen K.,"Zong, Peiyuan (57196403937); Lv, Tao (6603404297); Wang, Dawei (57219252430); Deng, Zizhuang (57219256299); Liang, Ruigang (57205634095); Chen, Kai (57051675000)",57196403937; 6603404297; 57219252430; 57219256299; 57205634095; 57051675000,fuzzguard: filtering out unreachable inputs in directed grey-box fuzzing through deep learning,2020,Proceedings of the 29th USENIX Security Symposium,,,,2255,2269,14,130,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091023244&partnerID=40&md5=c068300947bc51a4c616e9968dbc74b8,"recently, directed grey-box fuzzing (dgf) becomes popular in the field of software testing. different from coverage-based fuzzing whose goal is to increase code coverage for triggering more bugs, dgf is designed to check whether a piece of potentially buggy code (e.g., string operations) really contains a bug. ideally, all the inputs generated by dgf should reach the target buggy code until triggering the bug. it is a waste of time when executing with unreachable inputs. unfortunately, in real situations, large numbers of the generated inputs cannot let a program execute to the target, greatly impacting the efficiency of fuzzing, especially when the buggy code is embedded in the code guarded by various constraints. in this paper, we propose a deep-learning-based approach to predict the reachability of inputs (i.e., miss the target or not) before executing the target program, helping dgf filtering out the unreachable ones to boost the performance of fuzzing. to apply deep learning with dgf, we design a suite of new techniques (e.g., step-forwarding approach, representative data selection) to solve the problems of unbalanced labeled data and insufficient time in the training process. further, we implement the proposed approach called fuzzguard and equip it with the state-of-the-art dgf (e.g., aflgo). evaluations on 45 real vulnerabilities show that fuzzguard boosts the fuzzing efficiency of the vanilla aflgo up to 17.1×. finally, to understand the key features learned by fuzzguard, we illustrate their connection with the constraints in the programs. © 2020 by the usenix association. all rights reserved.",,codes (symbols); efficiency; software testing; code coverage; data selection; key feature; learning-based approach; reachability; real situation; state of the art; training process; deep learning,Conference paper,Final,,Scopus,2-s2.0-85091023244,fuzzguard filtering out unreachable inputs in directed greybox fuzzing through deep learning recently directed greybox fuzzing dgf becomes popular in the field of software testing different from coveragebased fuzzing whose goal is to increase code coverage for triggering more bugs dgf is designed to check whether a piece of potentially buggy code eg string operations really contains a bug ideally all the inputs generated by dgf should reach the target buggy code until triggering the bug it is a waste of time when executing with unreachable inputs unfortunately in real situations large numbers of the generated inputs cannot let a program execute to the target greatly impacting the efficiency of fuzzing especially when the buggy code is embedded in the code guarded by various constraints in this paper we propose a deeplearningbased approach to predict the reachability of inputs ie miss the target or not before executing the target program helping dgf filtering out the unreachable ones to boost the performance of fuzzing to apply deep learning with dgf we design a suite of new techniques eg stepforwarding approach representative data selection to solve the problems of unbalanced labeled data and insufficient time in the training process further we implement the proposed approach called fuzzguard and equip it with the stateoftheart dgf eg aflgo evaluations on  real vulnerabilities show that fuzzguard boosts the fuzzing efficiency of the vanilla aflgo up to  finally to understand the key features learned by fuzzguard we illustrate their connection with the constraints in the programs   by the usenix association all rights reserved,[''],6,21.666666666666668
scopus,Zivkovic T.; Nikolic B.; Simic V.; Pamucar D.; Bacanin N.,"Zivkovic, Tamara (6701358907); Nikolic, Bosko (7006055343); Simic, Vladimir (7005545253); Pamucar, Dragan (54080216100); Bacanin, Nebojsa (37028223900)",6701358907; 7006055343; 7005545253; 54080216100; 37028223900,software defects prediction by metaheuristics tuned extreme gradient boosting and analysis based on shapley additive explanations,2023,Applied Soft Computing,146,,110659,,,,52,10.1016/j.asoc.2023.110659,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167436767&doi=10.1016%2fj.asoc.2023.110659&partnerID=40&md5=89c419a568f152f6211a5a1a0099d7ff,"software testing represents a crucial component of software development, and it is usually making the difference between successful and failed projects. although it is extremely important, due to the fast pace and short deadlines of contemporary projects it is often neglected or not detailed enough due to the lack of available time, leading to the potential loss of reputation, private users’ data, money, and even lives in some circumstances. in such situations, it would be vital to have the option to predict what modules are error-prone according to the collection of software metrics, and to focus testing on them, and that task is a typical classification task. machine learning models have been frequently employed within a wide range of classification problems with significant success, and this paper proposes extreme gradient boosting (xgboost) model to execute the defect prediction task. a modified variant of the well-known reptile search optimization algorithm has been suggested to carry out the calibrating of the xgboost hyperparameters. the enhanced algorithm was named harsa and evaluated on the collection of challenging cec2019 benchmark functions, where it exhibited excellent performance. later, the introduced xgboost model that uses the proposed algorithm has been evaluated on two benchmark software testing datasets, and the simulation outcomes have been compared to other powerful swarm intelligence metaheuristics that were used in the identical experimental environment, where the proposed approach attained superior classification accuracy on both datasets. finally, shapley additive explanations analysis was conducted to discover the impact of various software metrics on the classification results. © 2023 elsevier b.v.",metaheuristics optimization; reptile search algorithm; software defect prediction; software testing; xgboost,adaptive boosting; additives; benchmarking; classification (of information); defects; forecasting; heuristic algorithms; optimization; software design; gradient boosting; metaheuristic; metaheuristic optimization; reptile search algorithm; search algorithms; shapley; software defect prediction; software metrics; software testings; xgboost; software testing,Article,Final,,Scopus,2-s2.0-85167436767,software defects prediction by metaheuristics tuned extreme gradient boosting and analysis based on shapley additive explanations software testing represents a crucial component of software development and it is usually making the difference between successful and failed projects although it is extremely important due to the fast pace and short deadlines of contemporary projects it is often neglected or not detailed enough due to the lack of available time leading to the potential loss of reputation private users data money and even lives in some circumstances in such situations it would be vital to have the option to predict what modules are errorprone according to the collection of software metrics and to focus testing on them and that task is a typical classification task machine learning models have been frequently employed within a wide range of classification problems with significant success and this paper proposes extreme gradient boosting xgboost model to execute the defect prediction task a modified variant of the wellknown reptile search optimization algorithm has been suggested to carry out the calibrating of the xgboost hyperparameters the enhanced algorithm was named harsa and evaluated on the collection of challenging cec benchmark functions where it exhibited excellent performance later the introduced xgboost model that uses the proposed algorithm has been evaluated on two benchmark software testing datasets and the simulation outcomes have been compared to other powerful swarm intelligence metaheuristics that were used in the identical experimental environment where the proposed approach attained superior classification accuracy on both datasets finally shapley additive explanations analysis was conducted to discover the impact of various software metrics on the classification results   elsevier bv,"['metaheuristics optimization', 'reptile search algorithm', 'software defect prediction', 'software testing', 'xgboost']",3,17.333333333333332
scopus,Zhu X.; Zhou W.; Han Q.-L.; Ma W.; Wen S.; Xiang Y.,"Zhu, Xiaogang (57215327205); Zhou, Wei (57838023900); Han, Qing-Long (7202485252); Ma, Wanlun (57196052588); Wen, Sheng (37108669800); Xiang, Yang (57114147900)",57215327205; 57838023900; 7202485252; 57196052588; 37108669800; 57114147900,when software security meets large language models: a survey,2025,IEEE/CAA Journal of Automatica Sinica,12,2,,317,334,17,17,10.1109/JAS.2024.124971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213132111&doi=10.1109%2fJAS.2024.124971&partnerID=40&md5=171b7e072ad0c537ceff53551536bb24,"software security poses substantial risks to our society because software has become part of our life. numerous techniques have been proposed to resolve or mitigate the impact of software security issues. among them, software testing and analysis are two of the critical methods, which significantly benefit from the advancements in deep learning technologies. due to the successful use of deep learning in software security, recently, researchers have explored the potential of using large language models (llms) in this area. in this paper, we systematically review the results focusing on llms in software security. we analyze the topics of fuzzing, unit test, program repair, bug reproduction, data-driven bug detection, and bug triage. we deconstruct these techniques into several stages and analyze how llms can be used in the stages. we also discuss the future directions of using llms in software security, including the future directions for the existing use of llms and extensions from conventional deep learning research. © 2014 chinese association of automation.",large language models (llms); software analysis; software security; software testing,language model; large language model; learning technology; security issues; software analysis; software security; software testing and analysis; software testings; test projects; unit tests; software testing,Article,Final,,Scopus,2-s2.0-85213132111,when software security meets large language models a survey software security poses substantial risks to our society because software has become part of our life numerous techniques have been proposed to resolve or mitigate the impact of software security issues among them software testing and analysis are two of the critical methods which significantly benefit from the advancements in deep learning technologies due to the successful use of deep learning in software security recently researchers have explored the potential of using large language models llms in this area in this paper we systematically review the results focusing on llms in software security we analyze the topics of fuzzing unit test program repair bug reproduction datadriven bug detection and bug triage we deconstruct these techniques into several stages and analyze how llms can be used in the stages we also discuss the future directions of using llms in software security including the future directions for the existing use of llms and extensions from conventional deep learning research   chinese association of automation,"['large language models (llms)', 'software analysis', 'software security', 'software testing']",1,17.0
scopus,Krichen M.,"Krichen, Moez (8973115500)",8973115500,a survey on formal verification and validation techniques for internet of things,2023,Applied Sciences (Switzerland),13,14,8122,,,,48,10.3390/app13148122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166181133&doi=10.3390%2fapp13148122&partnerID=40&md5=db224dc684a28fb864b27cf2ae93dad1,"the internet of things (iot) has brought about a new era of connected devices and systems, with applications ranging from healthcare to transportation. however, the reliability and security of these systems are critical concerns that must be addressed to ensure their safe and effective operation. this paper presents a survey of formal verification and validation (fv&v) techniques for iot systems, with a focus on the challenges and open issues in this field. we provide an overview of formal methods and testing techniques for the iot and discuss the state explosion problem and techniques to address it. we also examined the use of ai in software testing and describe examples of tools that use ai in this context. finally, we discuss the challenges and open issues in fv&v for the iot and present possible future directions for research. this survey paper aimed to provide a comprehensive understanding of the current state of fv&v techniques for iot systems and to highlight areas for further research and development. © 2023 by the author.",formal verification; internet of things; testing techniques; validation,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85166181133,a survey on formal verification and validation techniques for internet of things the internet of things iot has brought about a new era of connected devices and systems with applications ranging from healthcare to transportation however the reliability and security of these systems are critical concerns that must be addressed to ensure their safe and effective operation this paper presents a survey of formal verification and validation fvv techniques for iot systems with a focus on the challenges and open issues in this field we provide an overview of formal methods and testing techniques for the iot and discuss the state explosion problem and techniques to address it we also examined the use of ai in software testing and describe examples of tools that use ai in this context finally we discuss the challenges and open issues in fvv for the iot and present possible future directions for research this survey paper aimed to provide a comprehensive understanding of the current state of fvv techniques for iot systems and to highlight areas for further research and development   by the author,"['formal verification', 'internet of things', 'testing techniques', 'validation']",3,16.0
scopus,García de la Barrera A.; García-Rodríguez de Guzmán I.; Polo M.; Piattini M.,"García de la Barrera, Antonio (59454976500); García-Rodríguez de Guzmán, Ignacio (6602985950); Polo, Macario (7005519744); Piattini, Mario (7004203473)",59454976500; 6602985950; 7005519744; 7004203473,quantum software testing: state of the art,2023,Journal of Software: Evolution and Process,35,4,e2419,,,,46,10.1002/smr.2419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121582526&doi=10.1002%2fsmr.2419&partnerID=40&md5=3124e629e7b4711f63a17373a0a6bbf1,"quantum computing is expected to exponentially outperform classic computing on a broad set of problems, including encryption, machine learning, and simulations. it has an impact yet to explore on all software lifecycle's processes and techniques. testing quantum software raises a significant number of challenges due to the unique properties of quantum physics—such as superposition and entanglementand the stochastic behavior of quantum systems. it is, therefore, an open research issue. in this work, we offer a systematic mapping study of quantum software testing engineering, presenting a comprehensive view of the current state of the art. the main identified trends in testing techniques are (1) the statistic approaches based on repeated measurements and (2) the use of hoare-like logics to reason about software correctness. another relevant line of research is reversible circuit testing, which is partially applicable to quantum software unitary testing. finally, we have observed a flourishing of secondary studies and frameworks supporting testing processes from 2018 onwards. © 2021 the authors. journal of software: evolution and process published by john wiley & sons ltd.",,computation theory; cryptography; life cycle; quantum optics; stochastic systems; testing; life-cycle process; machine simulation; machine-learning; property; quantum computing; software evolution; software life cycles; software process; software testings; state of the art; software testing,Review,Final,,Scopus,2-s2.0-85121582526,quantum software testing state of the art quantum computing is expected to exponentially outperform classic computing on a broad set of problems including encryption machine learning and simulations it has an impact yet to explore on all software lifecycles processes and techniques testing quantum software raises a significant number of challenges due to the unique properties of quantum physicssuch as superposition and entanglementand the stochastic behavior of quantum systems it is therefore an open research issue in this work we offer a systematic mapping study of quantum software testing engineering presenting a comprehensive view of the current state of the art the main identified trends in testing techniques are  the statistic approaches based on repeated measurements and  the use of hoarelike logics to reason about software correctness another relevant line of research is reversible circuit testing which is partially applicable to quantum software unitary testing finally we have observed a flourishing of secondary studies and frameworks supporting testing processes from  onwards   the authors journal of software evolution and process published by john wiley  sons ltd,[''],3,15.333333333333334
scopus,Guo Q.; Xie X.; Li Y.; Zhang X.; Liu Y.; Li X.; Shen C.,"Guo, Qianyu (57221017262); Xie, Xiaofei (55268560900); Li, Yi (55894800200); Zhang, Xiaoyu (57214915278); Liu, Yang (56911879800); Li, Xiaohong (57022407900); Shen, Chao (36446592900)",57221017262; 55268560900; 55894800200; 57214915278; 56911879800; 57022407900; 36446592900,audee: automated testing for deep learning frameworks,2020,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",,,9286000,486,498,12,83,10.1145/3324884.3416571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099256881&doi=10.1145%2f3324884.3416571&partnerID=40&md5=e2c2ee6028e1cf0162885592a3731aea,"deep learning (dl) has been applied widely, and the quality of dl system becomes crucial, especially for safety-critical applications. existing work mainly focuses on the quality analysis of dl models, but lacks attention to the underlying frameworks on which all dl models depend. in this work, we propose audee, a novel approach for testing dl frameworks and localizing bugs. audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. audee is able to detect three types of bugs: logical bugs, crashes and not-a-number (nan) errors. in particular, for logical bugs, audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., tensorflow and pytorch), which may indicate potential bugs in their implementations. for nan errors, audee adopts a heuristic-based approach to generate dnns that tend to output outliers (i.e., too large or small values), and these values are likely to produce nan. furthermore, audee leverages a causal-testing based technique to localize layers as well as parameters that cause inconsistencies or bugs. to evaluate the effectiveness of our approach, we applied audee on testing four dl frameworks, i.e., tensorflow, pytorch, cntk, and theano. we generate a large number of dnns which cover 25 widely-used apis in the four frameworks. the results demonstrate that audee is effective in detecting inconsistencies, crashes and nan errors. intotal, 26 unique unknown bugs were discovered, and 7 of them have already been confirmed or fixed by the developers. © 2020 acm.",bug detection; deep learning frameworks; deep learning testing,automation; errors; model structures; quality control; safety engineering; sodium compounds; software engineering; well testing; automated testing; learning frameworks; mutation strategy; not a numbers; reference check; safety critical applications; search-based; test case; deep learning,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85099256881,audee automated testing for deep learning frameworks deep learning dl has been applied widely and the quality of dl system becomes crucial especially for safetycritical applications existing work mainly focuses on the quality analysis of dl models but lacks attention to the underlying frameworks on which all dl models depend in this work we propose audee a novel approach for testing dl frameworks and localizing bugs audee adopts a searchbased approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures parameters weights and inputs audee is able to detect three types of bugs logical bugs crashes and notanumber nan errors in particular for logical bugs audee adopts a crossreference check to detect behavioural inconsistencies across multiple frameworks eg tensorflow and pytorch which may indicate potential bugs in their implementations for nan errors audee adopts a heuristicbased approach to generate dnns that tend to output outliers ie too large or small values and these values are likely to produce nan furthermore audee leverages a causaltesting based technique to localize layers as well as parameters that cause inconsistencies or bugs to evaluate the effectiveness of our approach we applied audee on testing four dl frameworks ie tensorflow pytorch cntk and theano we generate a large number of dnns which cover  widelyused apis in the four frameworks the results demonstrate that audee is effective in detecting inconsistencies crashes and nan errors intotal  unique unknown bugs were discovered and  of them have already been confirmed or fixed by the developers   acm,"['bug detection', 'deep learning frameworks', 'deep learning testing']",6,13.833333333333334
scopus,Xu J.; Wang F.; Ai J.,"Xu, Jiaxi (57216952150); Wang, Fei (58599100300); Ai, Jun (57214531958)",57216952150; 58599100300; 57214531958,defect prediction with semantics and context features of codes based on graph representation learning,2021,IEEE Transactions on Reliability,70,2,9290043,613,625,12,69,10.1109/TR.2020.3040191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097938676&doi=10.1109%2fTR.2020.3040191&partnerID=40&md5=b22c0b47a913ba7342b5bd46c1e6261a,"to optimize the process of software testing and to improve software quality and reliability, many attempts have been made to develop more effective methods for predicting software defects. previous work on defect prediction has used machine learning and artificial software metrics. unfortunately, artificial metrics are unable to represent the features of syntactic, semantic, and context information of defective modules. in this article, therefore, we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning. graph neural networks are also leveraged to capture the latent defect information of defective subtrees, which are pruned based on a fix-inducing change. to validate the proposed approach for predicting defects, we define mining rules based on the github workflow and collect 6052 defects from 307 projects. the experiments indicate that the proposed approach performs better than the state-of-the-art approach and five traditional machine learning baselines. an ablation study shows that the information about code concepts leads to a significant increase in accuracy. © 1963-2012 ieee.",deep learning; defect prediction; graph representation learning; software defect dataset; software engineering,computer software selection and evaluation; forecasting; graph structures; machine learning; semantics; software quality; software reliability; software testing; syntactics; trees (mathematics); abstract syntax trees; context features; context information; defect prediction; graph neural networks; graph representation; software metrics; state-of-the-art approach; defects,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85097938676,defect prediction with semantics and context features of codes based on graph representation learning to optimize the process of software testing and to improve software quality and reliability many attempts have been made to develop more effective methods for predicting software defects previous work on defect prediction has used machine learning and artificial software metrics unfortunately artificial metrics are unable to represent the features of syntactic semantic and context information of defective modules in this article therefore we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning graph neural networks are also leveraged to capture the latent defect information of defective subtrees which are pruned based on a fixinducing change to validate the proposed approach for predicting defects we define mining rules based on the github workflow and collect  defects from  projects the experiments indicate that the proposed approach performs better than the stateoftheart approach and five traditional machine learning baselines an ablation study shows that the information about code concepts leads to a significant increase in accuracy    ieee,"['deep learning', 'defect prediction', 'graph representation learning', 'software defect dataset', 'software engineering']",5,13.8
scopus,Kukushkin K.; Ryabov Y.; Borovkov A.,"Kukushkin, Kuzma (57208472576); Ryabov, Yury (57200114015); Borovkov, Alexey (8840090300)",57208472576; 57200114015; 8840090300,digital twins: a systematic literature review based on data analysis and topic modeling,2022,Data,7,12,173,,,,54,10.3390/data7120173,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144595103&doi=10.3390%2fdata7120173&partnerID=40&md5=8626b57e7ee02faf6715cefab4d55af8,"the digital twin has recently become a popular topic in research related to manufacturing, such as industry 4.0, the industrial internet of things, and cyber-physical systems. in addition, digital twins are the focus of several research areas: construction, urban management, digital transformation of the economy, medicine, virtual reality, software testing, and others. the concept is not yet fully defined, its scope seems unlimited, and the topic is relatively new; all this can present a barrier to research. the main goal of this paper is to develop a proper methodology for visualizing the digital-twin science landscape using modern bibliometric tools, text-mining and topic-modeling, based on machine learning models—latent dirichlet allocation (lda) and bertopic (bidirectional encoder representations from transformers). the scope of the study includes 8693 publications on the topic selected from the scopus database, published between january 1993 and september 2022. keyword co-occurrence analysis and topic-modeling indicate that studies on digital twins are still in the early stage of development. at the same time, the core of the topic is growing, and some topic clusters are emerging. more than 100 topics can be identified; the most popular and fastest-growing topic is ‘digital twins of industrial robots, production lines and objects.’ further efforts are needed to verify the proposed methodology, which can be achieved by analyzing other research fields. © 2022 by the authors.",bertopic; bibliometrics; data analysis; digital twin; lda model; machine learning; systematic literature review; topic-modeling,data handling; e-learning; embedded systems; industrial research; industrial robots; information analysis; software testing; statistics; virtual reality; allocation model; bertopic; bibliometric; cyber-physical systems; data analysis models; latent dirichlet allocation; latent dirichlet allocation model; machine-learning; systematic literature review; topic modeling; machine learning,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144595103,digital twins a systematic literature review based on data analysis and topic modeling the digital twin has recently become a popular topic in research related to manufacturing such as industry  the industrial internet of things and cyberphysical systems in addition digital twins are the focus of several research areas construction urban management digital transformation of the economy medicine virtual reality software testing and others the concept is not yet fully defined its scope seems unlimited and the topic is relatively new all this can present a barrier to research the main goal of this paper is to develop a proper methodology for visualizing the digitaltwin science landscape using modern bibliometric tools textmining and topicmodeling based on machine learning modelslatent dirichlet allocation lda and bertopic bidirectional encoder representations from transformers the scope of the study includes  publications on the topic selected from the scopus database published between january  and september  keyword cooccurrence analysis and topicmodeling indicate that studies on digital twins are still in the early stage of development at the same time the core of the topic is growing and some topic clusters are emerging more than  topics can be identified the most popular and fastestgrowing topic is digital twins of industrial robots production lines and objects further efforts are needed to verify the proposed methodology which can be achieved by analyzing other research fields   by the authors,"['bertopic', 'bibliometrics', 'data analysis', 'digital twin', 'lda model', 'machine learning', 'systematic literature review', 'topic-modeling']",4,13.5
scopus,Xie X.; Li T.; Wang J.; Ma L.; Guo Q.; Juefei-Xu F.; Liu Y.,"Xie, Xiaofei (55268560900); Li, Tianlin (57218764226); Wang, Jian (57221358273); Ma, Lei (55479591700); Guo, Qing (57191163500); Juefei-Xu, Felix (54911989900); Liu, Yang (56911879800)",55268560900; 57218764226; 57221358273; 55479591700; 57191163500; 54911989900; 56911879800,npc: neuron path coverage via characterizing decision logic of deep neural networks,2022,ACM Transactions on Software Engineering and Methodology,31,3,47,,,,54,10.1145/3490489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130728492&doi=10.1145%2f3490489&partnerID=40&md5=4a2b36fff06c0dab5095153ffb174652,"deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. however, the quality of deep neural networks (dnns) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of dnns. however, due to the blackbox nature of dnn, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. the relationship between the structural coverage and the decision logic of dnns is unknown. moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and dnn defect detection, which further posts concerns on what a suitable dnn testing criterion should be.in this article, we propose the interpretable coverage criteria through constructing the decision structure of a dnn. mirroring the control flow graph of the traditional program, we first extract a decision graph from a dnn based on its interpretation, where a path of the decision graph represents a decision logic of the dnn. based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. the higher the path coverage, the more diverse decision logic the dnn is expected to be explored. our large-scale evaluation results demonstrate that: the path in the decision graph is effective in characterizing the decision of the dnn, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality. © 2022 association for computing machinery.",deep learning testing; model interpretation; testing coverage criteria,computer circuits; data flow analysis; flow graphs; graphic methods; safety engineering; safety testing; software testing; audio-recognition; coverage criteria; decision graphs; decision logic; deep learning testing; different domains; images classification; model interpretations; path coverage; testing coverage criterion; deep neural networks,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85130728492,npc neuron path coverage via characterizing decision logic of deep neural networks deep learning has recently been widely applied to many applications across different domains eg image classification and audio recognition however the quality of deep neural networks dnns still raises concerns in the practical operational environment which calls for systematic testing especially in safetycritical scenarios inspired by software testing a number of structural coverage criteria are designed and proposed to measure the test adequacy of dnns however due to the blackbox nature of dnn the existing structural coverage criteria are difficult to interpret making it hard to understand the underlying principles of these criteria the relationship between the structural coverage and the decision logic of dnns is unknown moreover recent studies have further revealed the nonexistence of correlation between the structural coverage and dnn defect detection which further posts concerns on what a suitable dnn testing criterion should bein this article we propose the interpretable coverage criteria through constructing the decision structure of a dnn mirroring the control flow graph of the traditional program we first extract a decision graph from a dnn based on its interpretation where a path of the decision graph represents a decision logic of the dnn based on the control flow and data flow of the decision graph we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic the higher the path coverage the more diverse decision logic the dnn is expected to be explored our largescale evaluation results demonstrate that the path in the decision graph is effective in characterizing the decision of the dnn and the proposed coverage criteria are also sensitive with errors including natural errors and adversarial examples and strongly correlate with the output impartiality    association for computing machinery,"['deep learning testing', 'model interpretation', 'testing coverage criteria']",4,13.5
scopus,Mehta S.; Patnaik K.S.,"Mehta, Sweta (57222256908); Patnaik, K. Sridhar (24438167800)",57222256908; 24438167800,improved prediction of software defects using ensemble machine learning techniques,2021,Neural Computing and Applications,33,16,,10551,10562,11,64,10.1007/s00521-021-05811-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102026578&doi=10.1007%2fs00521-021-05811-3&partnerID=40&md5=b3a60d250a98b993fd618e9e0ee5b521,"software testing process is a crucial part in software development. generally the errors made by developers get fixed at a later stage of the software development process. this increases the impact of the defect. to prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. defect prediction process involves classification of software modules into defect prone and non-defect prone. this paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. in this research work, various software metrics are evaluated using feature selection techniques such as recursive feature elimination (rfe), correlation-based feature selection, lasso, ridge, elasticnet and boruta. logistic regression, decision trees, k-nearest neighbor, support vector machines and ensemble learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. the proposed model uses combination of partial least square (pls) regression and rfe for dimension reduction which is further combined with synthetic minority oversampling technique due to the imbalanced nature of the used datasets. it has been observed that xgboost and stacking ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work. © 2021, the author(s), under exclusive licence to springer-verlag london ltd. part of springer nature.",data imbalance; defect prediction; dimension reduction; machine learning algorithms; stacking ensemble classifier; xgboost,decision trees; defects; feature extraction; forecasting; learning systems; logistic regression; nearest neighbor search; predictive analytics; software testing; support vector machines; support vector regression; ensemble techniques; k-nearest neighbors; machine learning techniques; partial least-square regression; recursive feature elimination; selection techniques; software development process; synthetic minority over-sampling techniques; software design,Article,Final,,Scopus,2-s2.0-85102026578,improved prediction of software defects using ensemble machine learning techniques software testing process is a crucial part in software development generally the errors made by developers get fixed at a later stage of the software development process this increases the impact of the defect to prevent this defects need to be predicted during the initial days of the software development which in turn helps in efficient utilization of the testing resources defect prediction process involves classification of software modules into defect prone and nondefect prone this paper aims to reduce the impact of two major issues faced during defect prediction ie data imbalance and high dimensionality of the defect datasets in this research work various software metrics are evaluated using feature selection techniques such as recursive feature elimination rfe correlationbased feature selection lasso ridge elasticnet and boruta logistic regression decision trees knearest neighbor support vector machines and ensemble learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and nondefect prone the proposed model uses combination of partial least square pls regression and rfe for dimension reduction which is further combined with synthetic minority oversampling technique due to the imbalanced nature of the used datasets it has been observed that xgboost and stacking ensemble technique gave best results for all the datasets with defect prediction accuracy more than  as compared to algorithms used in the research work   the authors under exclusive licence to springerverlag london ltd part of springer nature,"['data imbalance', 'defect prediction', 'dimension reduction', 'machine learning algorithms', 'stacking ensemble classifier', 'xgboost']",5,12.8
scopus,Jin C.,"Jin, Cong (8704560000)",8704560000,cross-project software defect prediction based on domain adaptation learning and optimization,2021,Expert Systems with Applications,171,,114637,,,,62,10.1016/j.eswa.2021.114637,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100588747&doi=10.1016%2fj.eswa.2021.114637&partnerID=40&md5=68d1f964aa6187d44c3844792ae7a45e,"software defect prediction (sdp) is very helpful for optimizing the resource allocation of software testing and improving the quality of software products. the cross-project defect prediction (cpdp) model based on machine learning is first learned through the existing training data with sufficient number and defect labels on one project, and then used to predict the defect labels of another new project with insufficient number and fewer labeled data. however, its prediction performance has a large gap compared with the within-project defect prediction (wpdp) model. the main reason is that there are usually differences between the distributions of training data in different software projects, and it has a greater impact on the prediction performance of the cpdp model. to solve this problem, the kernel twin support vector machines (ktsvms) is used to implement domain adaptation (da) to match the distributions of training data for different projects. moreover, ktsvms with da function (called da-ktsvm) is further used as the cpdp model in this paper. since the parameters of da-ktsvm have an impact on its predictive performance, these parameters are optimized by an improved quantum particle swarm optimization algorithm (iqpso), and the optimized da-ktsvm is called as da-ktsvmo. in order to confirm the effectiveness of da-ktsvmo, some experiments are implemented on 17 open source software projects. experimental results and analysis show that da-ktsvmo can not only achieve better prediction performance than other cpdp models compared, but also achieve almost the same or better compared performance than wpdp models when the training sample data is sufficient. in addition, da-ktsvmo can make better use of existing sufficient data knowledge and realize the reuse of defective data to improve the prediction performance of da-ktsvmo. © 2021 elsevier ltd",cross-project defect prediction; domain adaptation; improved quantum particle swarm optimization; optimization; software defect prediction,defects; forecasting; open systems; particle swarm optimization (pso); software quality; software testing; support vector machines; domain adaptation learning; open source software projects; prediction performance; predictive performance; quality of softwares; quantum particle swarm optimization algorithm; software defect prediction; twin support vector machines; open source software,Article,Final,,Scopus,2-s2.0-85100588747,crossproject software defect prediction based on domain adaptation learning and optimization software defect prediction sdp is very helpful for optimizing the resource allocation of software testing and improving the quality of software products the crossproject defect prediction cpdp model based on machine learning is first learned through the existing training data with sufficient number and defect labels on one project and then used to predict the defect labels of another new project with insufficient number and fewer labeled data however its prediction performance has a large gap compared with the withinproject defect prediction wpdp model the main reason is that there are usually differences between the distributions of training data in different software projects and it has a greater impact on the prediction performance of the cpdp model to solve this problem the kernel twin support vector machines ktsvms is used to implement domain adaptation da to match the distributions of training data for different projects moreover ktsvms with da function called daktsvm is further used as the cpdp model in this paper since the parameters of daktsvm have an impact on its predictive performance these parameters are optimized by an improved quantum particle swarm optimization algorithm iqpso and the optimized daktsvm is called as daktsvmo in order to confirm the effectiveness of daktsvmo some experiments are implemented on  open source software projects experimental results and analysis show that daktsvmo can not only achieve better prediction performance than other cpdp models compared but also achieve almost the same or better compared performance than wpdp models when the training sample data is sufficient in addition daktsvmo can make better use of existing sufficient data knowledge and realize the reuse of defective data to improve the prediction performance of daktsvmo   elsevier ltd,"['cross-project defect prediction', 'domain adaptation', 'improved quantum particle swarm optimization', 'optimization', 'software defect prediction']",5,12.4
scopus,Li Z.; Wang C.; Liu Z.; Wang H.; Chen D.; Wang S.; Gao C.,"Li, Zongjie (57221598146); Wang, Chaozheng (57219285770); Liu, Zhibo (57218364577); Wang, Haoxuan (57849500200); Chen, Dong (58046110600); Wang, Shuai (57190181124); Gao, Cuiyun (57189036288)",57221598146; 57219285770; 57218364577; 57849500200; 58046110600; 57190181124; 57189036288,cctest: testing and repairing code completion systems,2023,Proceedings - International Conference on Software Engineering,,,,1238,1250,12,35,10.1109/ICSE48619.2023.00110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163941249&doi=10.1109%2fICSE48619.2023.00110&partnerID=40&md5=43b7e845ba21fcdd3d967372d76f266c,"code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (llms). to date, visible llm-based code completion frameworks such as github copilot and gpt are trained using deep learning over vast quantities of unstructured text and open source code. as the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. in contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. this research proposes cctest, a framework to test and repair code completion systems in black-box settings. cctest features a set of novel mutation strategies, namely program structure-consistent (psc) mutations, to generate mutated code completion inputs. then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. moreover, cctest repairs the code completion outputs by selecting the output that mostly reflects the 'average' appearance of all output cases, as the final output of the code completion systems. with around 18k test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular llm-based code completion systems. with repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to bleu score and levenshtein edit similarity. © 2023 ieee.",,codes (symbols); deep learning; open systems; repair; software design; code completions; in-buildings; language model; model-based opc; open-source code; programming tasks; real-world; software-systems; text sources; unstructured texts; open source software,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85163941249,cctest testing and repairing code completion systems code completion a highly valuable topic in the software development domain has been increasingly promoted for use by recent advances in large language models llms to date visible llmbased code completion frameworks such as github copilot and gpt are trained using deep learning over vast quantities of unstructured text and open source code as the paramount component and the cornerstone in daily programming tasks code completion has largely boosted professionals efficiency in building realworld software systems in contrast to this flourishing market we find that code completion systems often output suspicious results and to date an automated testing and enhancement framework for code completion systems is not available this research proposes cctest a framework to test and repair code completion systems in blackbox settings cctest features a set of novel mutation strategies namely program structureconsistent psc mutations to generate mutated code completion inputs then it detects inconsistent outputs representing possibly erroneous cases from all the completed code cases moreover cctest repairs the code completion outputs by selecting the output that mostly reflects the average appearance of all output cases as the final output of the code completion systems with around k test inputs we detected  inputs that can trigger erroneous cases with a true positive rate of  from eight popular llmbased code completion systems with repairing we show that the accuracy of code completion systems is notably increased by  and  with respect to bleu score and levenshtein edit similarity   ieee,[''],3,11.666666666666666
scopus,Borandag E.,"Borandag, Emin (57063310500)",57063310500,software fault prediction using an rnn-based deep learning approach and ensemble machine learning techniques,2023,Applied Sciences (Switzerland),13,3,1639,,,,35,10.3390/app13031639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148017033&doi=10.3390%2fapp13031639&partnerID=40&md5=39c0baf9607c52c0128dd3415ff7ecab,"alongside the modern software development life cycle approaches, software testing has gained more importance and has become an area researched actively within the software engineering discipline. in this study, machine learning and deep learning-related software fault predictions were made through a data set named sfp xp-tdd, which was created using three different developed software projects. a data set of five different classifiers widely used in the literature and their rotation forest classifier ensemble versions were trained and tested using this data set. numerous publications in the literature discussed software fault predictions through ml algorithms addressing solutions to different problems. some of these articles indicated the usage of feature selection algorithms to improve classification performance, while others reported operating ensemble machine learning algorithms for software fault predictions. besides, a detailed literature review revealed that there were few studies involving software fault prediction with dl algorithms due to the small sample sizes in the data sets and the low success rates in the tests performed on these datasets. as a result, the major contribution of this research was to statistically demonstrate that dl algorithms outperformed ml algorithms in data sets with large sample values via employing three separate software fault prediction datasets. the experimental outcomes of a model that includes a layer of recurrent neural networks (rnns) were enclosed within this study. alongside the aforementioned and generated data sets, the study also utilized the eclipse and apache active mq data sets in to test the effectiveness of the proposed deep learning method. © 2023 by the author.",deep learning; ensemble machine learning; software fault prediction,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85148017033,software fault prediction using an rnnbased deep learning approach and ensemble machine learning techniques alongside the modern software development life cycle approaches software testing has gained more importance and has become an area researched actively within the software engineering discipline in this study machine learning and deep learningrelated software fault predictions were made through a data set named sfp xptdd which was created using three different developed software projects a data set of five different classifiers widely used in the literature and their rotation forest classifier ensemble versions were trained and tested using this data set numerous publications in the literature discussed software fault predictions through ml algorithms addressing solutions to different problems some of these articles indicated the usage of feature selection algorithms to improve classification performance while others reported operating ensemble machine learning algorithms for software fault predictions besides a detailed literature review revealed that there were few studies involving software fault prediction with dl algorithms due to the small sample sizes in the data sets and the low success rates in the tests performed on these datasets as a result the major contribution of this research was to statistically demonstrate that dl algorithms outperformed ml algorithms in data sets with large sample values via employing three separate software fault prediction datasets the experimental outcomes of a model that includes a layer of recurrent neural networks rnns were enclosed within this study alongside the aforementioned and generated data sets the study also utilized the eclipse and apache active mq data sets in to test the effectiveness of the proposed deep learning method   by the author,"['deep learning', 'ensemble machine learning', 'software fault prediction']",3,11.666666666666666
